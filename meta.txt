
# src/classifiers/__init__.py
"""Page classifiers."""

from .base_classifier import BaseClassifier
from .rule_based import RuleBasedClassifier
from .ai_classifier import AIClassifier

__all__ = ['BaseClassifier', 'RuleBasedClassifier', 'AIClassifier']


classifire/ai_classifier.py"""AI-based page classifier using Gemini."""

import google.generativeai as genai
from .base_classifier import BaseClassifier
from .rule_based import RuleBasedClassifier


class AIClassifier(BaseClassifier):
    """Classify pages using AI (Gemini)."""
    
    def __init__(self, api_key: str, model_name: str = "models/gemini-2.0-flash-exp"):
        """
        Initialize AI classifier.
        
        Args:
            api_key: Gemini API key
            model_name: Gemini model to use
        """
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.fallback_classifier = RuleBasedClassifier()
        print("âœ“ Gemini AI enabled for page classification")
    
    def is_product_page(self, url: str, markdown: str) -> bool:
        """
        Use AI to determine if page is a product page.
        
        Args:
            url: The page URL
            markdown: The page content
            
        Returns:
            True if product page, False otherwise
        """
        # Truncate content for speed
        content_sample = markdown[:3000]
        
        prompt = f"""
Analyze this webpage and determine if it's a PRODUCT CUSTOMIZATION PAGE.

A product customization page typically has:
- Base product information (price, dimensions, specs)
- Multiple customization categories (wood types, colors, accessories)
- Options with prices (e.g., "+$500", "Included")
- Images of different options
- Form-like structure for selecting options

URL: {url}

Content sample:
\"\"\"
{content_sample}
\"\"\"

Respond with ONLY "YES" or "NO".
"""
        
        try:
            response = self.model.generate_content(prompt)
            answer = response.text.strip().upper()
            return answer == "YES"
        except Exception as e:
            print(f"  âš  AI classification failed, using rules: {e}")
            return self.fallback_classifier.is_product_page(url, markdown)



classifier/base_classifier.py
"""Base classifier interface."""

from abc import ABC, abstractmethod


class BaseClassifier(ABC):
    """Abstract base class for page classifiers."""
    
    @abstractmethod
    def is_product_page(self, url: str, markdown: str) -> bool:
        """
        Determine if a page is a product customization page.
        
        Args:
            url: The page URL
            markdown: The page content in markdown
            
        Returns:
            True if product page, False otherwise
        """
        pass


classifier/rule_based.py
"""Rule-based page classifier."""

import re
from .base_classifier import BaseClassifier


class RuleBasedClassifier(BaseClassifier):
    """Classify pages using rule-based logic."""
    
    def __init__(self):
        self.url_indicators = [
            'inquiry', 'enquiry', 'customize', 'builder', 'configurator',
            'quote', 'quotation', 'build-your', 'design-your'
        ]
        
        self.content_indicators = [
            'base price',
            'customization',
            'choose your',
            'select your',
            'interior paneling',
            'exterior',
            'heater',
            'accessories',
            'add-ons',
            'options',
            'upgrade',
            'included',
            'dimensions:',
            'capacity:',
            'electrical requirements'
        ]
    
    def is_product_page(self, url: str, markdown: str) -> bool:
        """
        Check if page is a product/customization page using rules.
        
        Args:
            url: The page URL
            markdown: The page content
            
        Returns:
            True if product page, False otherwise
        """
        url_lower = url.lower()
        markdown_lower = markdown.lower()
        
        # Check URL indicators
        if any(indicator in url_lower for indicator in self.url_indicators):
            return True
        
        # Check content indicators
        indicator_count = sum(
            1 for indicator in self.content_indicators 
            if indicator in markdown_lower
        )
        
        # If page has multiple customization indicators
        if indicator_count >= 3:
            return True
        
        # Check for form-like structures (multiple images with prices)
        price_pattern_count = len(re.findall(r'\(\+?\$[\d,]+\)', markdown))
        if price_pattern_count >= 5:
            return True
        
        return False


core/config.py"""Configuration management for the scraper."""

import os
from dataclasses import dataclass
from typing import Optional
from dotenv import load_dotenv

load_dotenv()


@dataclass
class ScraperConfig:
    """Configuration for the web scraper."""
    
    # Target website
    base_url: str = "https://casarista.com/en/furniture/sofa-en/"
    
    # Crawling settings
    max_pages: int = 50
    max_depth: int = 3
    crawl_delay: float = 0.5  # seconds between requests
    
    # Scraping settings
    request_timeout: int = 15
    
    # AI settings
    use_ai_classification: bool = False
    gemini_api_key: Optional[str] = None
    gemini_model: str = "models/gemini-2.0-flash-exp"
    
    # Storage settings
    output_dir: str = "data/catalogs"
    output_filename: str = "product_catalog.json"
    
    # Jina AI settings
    jina_api_url: str = "https://r.jina.ai/"
    
    def __post_init__(self):
        """Initialize configuration from environment variables."""
        self.gemini_api_key = (
            os.getenv("GEMINI_API_KEY") or 
            os.getenv("GEMINAI_API_KEY")
        )
        
        # Create output directory if it doesn't exist
        os.makedirs(self.output_dir, exist_ok=True)
    
    @property
    def full_output_path(self) -> str:
        """Get the full path for output file."""
        return os.path.join(self.output_dir, self.output_filename)
    
    def validate(self) -> bool:
        """Validate configuration."""
        if self.use_ai_classification and not self.gemini_api_key:
            print("âš  Warning: AI classification enabled but no API key found")
            return False
        return True


core/scraper.py"""Main scraper orchestrator."""

import time
from typing import Dict, Optional, Set
from .config import ScraperConfig
from ..utils.http_client import HTTPClient
from ..extractors.link_extractor import LinkExtractor
from ..extractors.product_extractor import ProductExtractor
from ..classifiers.rule_based import RuleBasedClassifier
from ..classifiers.ai_classifier import AIClassifier
from ..crawlers.web_crawler import WebCrawler
from ..storage.json_storage import JSONStorage
from ..storage.csv_storage import CSVStorage
from ..storage.google_sheets import GoogleSheetsStorage
from ..storage.quotation_template import QuotationTemplate


class TheraluxeScraper:
    """Main scraper that orchestrates the crawling and extraction."""
    
    def __init__(self, config: ScraperConfig):
        """
        Initialize the scraper.
        
        Args:
            config: Scraper configuration
        """
        self.config = config
        
        # Initialize components
        self.http_client = HTTPClient(timeout=config.request_timeout)
        self.link_extractor = LinkExtractor()
        self.product_extractor = ProductExtractor()
        
        # Initialize classifier (AI or rule-based)
        if config.use_ai_classification and config.gemini_api_key:
            self.classifier = AIClassifier(
                api_key=config.gemini_api_key,
                model_name=config.gemini_model
            )
        else:
            if config.use_ai_classification:
                print("âš  Gemini API key not found, using rule-based classification")
            self.classifier = RuleBasedClassifier()
        
        # Initialize crawler
        self.crawler = WebCrawler(
            base_url=config.base_url,
            http_client=self.http_client,
            link_extractor=self.link_extractor,
            classifier=self.classifier,
            crawl_delay=config.crawl_delay
        )
        
        # Initialize storage
        self.json_storage = JSONStorage()
        self.csv_storage = CSVStorage()
        self.quotation_template = QuotationTemplate()
        self.google_sheets = None  # Initialized on demand
    
    def scrape_product(self, url: str) -> Optional[Dict]:
        """
        Scrape a single product page.
        
        Args:
            url: The product page URL
            
        Returns:
            Product data dictionary or None
        """
        print(f"\n{'â”€'*80}")
        print(f"Scraping product: {url}")
        print(f"{'â”€'*80}")
        
        # Scrape the page
        markdown = self.http_client.scrape_with_jina(url)
        
        if not markdown:
            return None
        
        # Extract product information
        product_name = self.product_extractor.extract_product_name(url, markdown)
        base_price = self.product_extractor.extract_base_price(markdown)
        customizations = self.product_extractor.extract_customizations(markdown)
        
        product_data = {
            "product_name": product_name,
            "url": url,
            "base_price": base_price,
            "customization_categories": list(customizations.keys()),
            "customizations": customizations,
            "total_customization_options": sum(len(opts) for opts in customizations.values())
        }
        
        # Print summary
        print(f"  Product: {product_name}")
        print(f"  Base Price: {base_price or 'Not found'}")
        print(f"  Categories: {len(customizations)}")
        print(f"  Total Options: {product_data['total_customization_options']}")
        
        return product_data
    
    def scrape_all_products(self) -> Dict[str, Dict]:
        """
        Main method: crawl website and scrape all products.
        
        Returns:
            Complete product catalog
        """
        print("\n" + "="*80)
        print("DYNAMIC PRODUCT CATALOG SCRAPER")
        print("="*80)
        
        # Step 1: Crawl and discover product pages
        product_urls = self.crawler.crawl(
            max_pages=self.config.max_pages,
            max_depth=self.config.max_depth
        )
        
        if not product_urls:
            print("\nâš  No product pages found!")
            return {}
        
        # Step 2: Scrape each product
        print("\n" + "="*80)
        print("SCRAPING PRODUCT DETAILS")
        print("="*80)
        
        catalog = {}
        
        for url in product_urls:
            product_data = self.scrape_product(url)
            
            if product_data:
                product_id = product_data['product_name'].lower().replace(' ', '_')
                catalog[product_id] = product_data
            
            # Delay between product scrapes
            time.sleep(1)
        
        return catalog
    
    def save_catalog(self, catalog: Dict, export_formats: list = ['json']):
        """
        Save catalog to file(s).
        
        Args:
            catalog: The catalog to save
            export_formats: List of formats ['json', 'csv', 'csv_prices', 'google_sheets', 'quotation']
        """
        import os
        
        for fmt in export_formats:
            if fmt == 'json':
                self.json_storage.save(catalog, self.config.full_output_path)
            
            elif fmt == 'csv':
                csv_path = self.config.full_output_path.replace('.json', '.csv')
                self.csv_storage.save_simple(catalog, csv_path)
            
            elif fmt == 'csv_prices':
                csv_path = self.config.full_output_path.replace('.json', '_with_prices.csv')
                self.csv_storage.save_with_prices(catalog, csv_path)
            
            elif fmt == 'quotation':
                quot_path = self.config.full_output_path.replace('.json', '_quotation_template.json')
                self.quotation_template.create(catalog, quot_path)
            
            elif fmt == 'google_sheets':
                # Initialize Google Sheets on demand
                if self.google_sheets is None:
                    credentials_file = os.getenv('GOOGLE_CREDENTIALS_FILE', 'credentials.json')
                    self.google_sheets = GoogleSheetsStorage(credentials_file)
                
                # Check if spreadsheet ID is provided
                spreadsheet_id = os.getenv('GOOGLE_SPREADSHEET_ID')
                
                if self.google_sheets.service:
                    self.google_sheets.save_catalog(
                        catalog,
                        spreadsheet_id=spreadsheet_id,
                        title="Theraluxe Product Catalog"
                    )
    
    def print_summary(self, catalog: Dict):
        """
        Print catalog summary.
        
        Args:
            catalog: The catalog to summarize
        """
        print("\n" + "="*80)
        print("CATALOG SUMMARY")
        print("="*80 + "\n")
        
        total_products = len(catalog)
        total_categories = sum(len(p['customization_categories']) for p in catalog.values())
        total_options = sum(p['total_customization_options'] for p in catalog.values())
        
        print(f"Total Products: {total_products}")
        print(f"Total Customization Categories: {total_categories}")
        print(f"Total Customization Options: {total_options}\n")
        
        for product_id, data in catalog.items():
            print(f"ğŸ“¦ {data['product_name']}")
            print(f"   Price: {data['base_price'] or 'N/A'}")
            print(f"   Categories: {len(data['customization_categories'])}")
            print(f"   Options: {data['total_customization_options']}")
            print(f"   URL: {data['url']}\n")


    
# /core/__init__.py
"""Core scraper components."""
import argparse
from .scraper import TheraluxeScraper
from .config import ScraperConfig

__all__ = ['TheraluxeScraper', 'ScraperConfig']



core/web_crawler.py"""Web crawler for discovering pages."""

import time
from typing import Set, Tuple
from ..utils.http_client import HTTPClient
from ..extractors.link_extractor import LinkExtractor
from ..classifiers.base_classifier import BaseClassifier


class WebCrawler:
    """Crawl website to discover pages and identify product pages."""
    
    def __init__(
        self,
        base_url: str,
        http_client: HTTPClient,
        link_extractor: LinkExtractor,
        classifier: BaseClassifier,
        crawl_delay: float = 0.5
    ):
        """
        Initialize web crawler.
        
        Args:
            base_url: The base URL to crawl
            http_client: HTTP client for making requests
            link_extractor: Link extractor for finding URLs
            classifier: Page classifier
            crawl_delay: Delay between requests in seconds
        """
        self.base_url = base_url
        self.http_client = http_client
        self.link_extractor = link_extractor
        self.classifier = classifier
        self.crawl_delay = crawl_delay
        
        self.visited_pages: Set[str] = set()
        self.product_pages: Set[str] = set()
    
    def crawl(self, max_pages: int = 50, max_depth: int = 3) -> Set[str]:
        """
        Crawl the website to discover product pages.
        
        Args:
            max_pages: Maximum number of pages to crawl
            max_depth: Maximum crawl depth
            
        Returns:
            Set of product page URLs
        """
        print("\n" + "="*80)
        print("CRAWLING WEBSITE TO DISCOVER PRODUCT PAGES")
        print("="*80 + "\n")
        
        to_visit = [(self.base_url, 0)]  # (url, depth)
        
        while to_visit and len(self.visited_pages) < max_pages:
            current_url, depth = to_visit.pop(0)
            
            # Skip if already visited
            if current_url in self.visited_pages:
                continue
            
            # Skip if too deep
            if depth > max_depth:
                continue
            
            print(f"ğŸ” Crawling [{len(self.visited_pages)+1}/{max_pages}]: {current_url}")
            
            # Scrape the page
            markdown = self.http_client.scrape_with_jina(current_url)
            
            if not markdown:
                self.visited_pages.add(current_url)
                continue
            
            self.visited_pages.add(current_url)
            
            # Check if it's a product page
            if self.classifier.is_product_page(current_url, markdown):
                self.product_pages.add(current_url)
                print(f"  âœ“ PRODUCT PAGE DETECTED: {current_url}")
            
            # Extract links for further crawling
            links = self.link_extractor.extract_from_markdown(markdown, current_url)
            
            # Add new links to visit queue
            for link in links:
                if link not in self.visited_pages and link not in [url for url, _ in to_visit]:
                    to_visit.append((link, depth + 1))
            
            # Be nice to the server
            time.sleep(self.crawl_delay)
        
        print(f"\nâœ“ Crawling complete!")
        print(f"  Pages visited: {len(self.visited_pages)}")
        print(f"  Product pages found: {len(self.product_pages)}")
        
        return self.product_pages

# src/crawlers/__init__.py
"""Web crawlers."""

from .web_crawler import WebCrawler

__all__ = ['WebCrawler']

# src/extractors/__init__.py
"""Data extractors."""

from .link_extractor import LinkExtractor
from .product_extractor import ProductExtractor

__all__ = ['LinkExtractor', 'ProductExtractor']

sxtractors/link_extractors.py
"""Extract links from markdown content."""

import re
from typing import Set
from ..utils.url_utils import URLUtils


class LinkExtractor:
    """Extract and process links from markdown content."""
    
    def __init__(self):
        self.url_utils = URLUtils()
    
    def extract_from_markdown(self, markdown: str, base_url: str) -> Set[str]:
        """
        Extract all links from markdown content.
        
        Args:
            markdown: The markdown content
            base_url: The base URL for making links absolute
            
        Returns:
            Set of cleaned, absolute URLs
        """
        links = set()
        
        # Find markdown links: [text](url)
        markdown_links = re.findall(r'\[([^\]]+)\]\(([^\)]+)\)', markdown)
        
        for text, url in markdown_links:
            # Skip invalid URLs
            if not self.url_utils.is_valid_url(url):
                continue
            
            # Make absolute URL
            url = self.url_utils.make_absolute(url, base_url)
            
            # Only keep links from the same domain
            if self.url_utils.is_same_domain(url, base_url):
                # Clean URL
                clean_url = self.url_utils.clean_url(url)
                links.add(clean_url)
        
        return links




extractors/product_extractor.py"""Extract product information from markdown content."""

import re
from typing import Dict, List, Optional
from urllib.parse import urlparse


class ProductExtractor:
    """Extract product details from scraped content."""
    
    def extract_product_name(self, url: str, markdown: str) -> str:
        """
        Extract product name from markdown or URL.
        
        Args:
            url: The product page URL
            markdown: The markdown content
            
        Returns:
            Product name
        """
        # Try to find title in markdown
        for line in markdown.split('\n'):
            line = line.strip()
            if line.startswith('# ') or line.startswith('## '):
                title = line.lstrip('#').strip()
                # Skip generic titles
                if len(title) > 3 and 'skip' not in title.lower() and 'content' not in title.lower():
                    return title
        
        # Fallback to URL
        path = urlparse(url).path
        name = path.strip('/').split('/')[-1]
        return name
    
    def extract_base_price(self, markdown: str) -> Optional[str]:
        """
        Extract base price from markdown.
        
        Args:
            markdown: The markdown content
            
        Returns:
            Base price string or None
        """
        price_match = re.search(
            r'Base Price:\s*\$[\d,]+(?:\.\d{2})?\s*(?:CAD|USD)?',
            markdown,
            re.IGNORECASE
        )
        if price_match:
            return price_match.group(0).replace('Base Price:', '').strip()
        return None
    
    def extract_customizations(self, markdown: str) -> Dict[str, List[Dict]]:
        """
        Extract all customization categories and their options.
        
        Args:
            markdown: The markdown content
            
        Returns:
            Dictionary of customization categories and options
        """
        lines = markdown.split('\n')
        customizations = {}
        current_category = None
        current_options = []
        
        for line in lines:
            line_stripped = line.strip()
            
            # Detect category headers
            category_match = re.match(r'^([A-Z][^:]+?):\s*\*?\s*$', line_stripped)
            
            if category_match:
                # Save previous category
                if current_category and current_options:
                    customizations[current_category] = current_options
                
                current_category = category_match.group(1).strip()
                current_options = []
                continue
            
            # Extract options from images
            if current_category:
                option = self._extract_image_option(line_stripped)
                if option:
                    current_options.append(option)
                    continue
                
                # Extract from checkboxes
                option = self._extract_checkbox_option(line_stripped)
                if option:
                    current_options.append(option)
        
        # Save last category
        if current_category and current_options:
            customizations[current_category] = current_options
        
        return self._clean_customizations(customizations)
    
    def _extract_image_option(self, line: str) -> Optional[Dict]:
        """Extract option from image markdown."""
        image_match = re.search(
            r'!\[(?:Image \d+:?\s*)?([^\]]+?)\s*(?:\(\+?\$[\d,]+\))?\]\(([^\)]+)\)',
            line
        )
        
        if not image_match:
            return None
        
        alt_text = image_match.group(1).strip()
        image_url = image_match.group(2).strip()
        
        # Skip tracking pixels
        if 'pixel.wp.com' in image_url or 'g.gif' in image_url:
            return None
        
        # Extract price
        price_match = re.search(r'\(\+?\$[\d,]+\)', alt_text)
        price = price_match.group(0).strip('()') if price_match else None
        
        # Clean label
        label = re.sub(r'\s*\(\+?\$[\d,]+\)\s*$', '', alt_text).strip()
        
        if label and len(label) > 2:
            return {
                "label": label,
                "price": price,
                "image": image_url
            }
        
        return None
    
    def _extract_checkbox_option(self, line: str) -> Optional[Dict]:
        """Extract option from checkbox markdown."""
        checkbox_match = re.match(r'^-\s*\[x?\]\s*(.+?)\s*\(\+?\$[\d,]+\)', line)
        
        if not checkbox_match:
            return None
        
        full_text = checkbox_match.group(0)
        label = checkbox_match.group(1).strip()
        
        price_match = re.search(r'\(\+?\$[\d,]+\)', full_text)
        price = price_match.group(0).strip('()') if price_match else None
        
        return {
            "label": label,
            "price": price,
            "image": None
        }
    
    def _clean_customizations(self, customizations: Dict) -> Dict:
        """Remove duplicates from customizations."""
        cleaned = {}
        
        for category, options in customizations.items():
            seen = {}
            unique = []
            
            for opt in options:
                label = opt['label'].strip()
                key = label.lower()
                
                # Skip very short labels
                if len(label) < 3:
                    continue
                
                if key in seen:
                    # Merge with existing option
                    idx = seen[key]
                    if opt['image'] and not unique[idx]['image']:
                        unique[idx]['image'] = opt['image']
                    if opt['price'] and not unique[idx]['price']:
                        unique[idx]['price'] = opt['price']
                else:
                    seen[key] = len(unique)
                    unique.append(opt)
            
            if unique:
                cleaned[category] = unique
        
        return cleaned



# src/storage/__init__.py
"""Storage backends."""

from .json_storage import JSONStorage
from .csv_storage import CSVStorage
from .google_sheets import GoogleSheetsStorage
from .quotation_template import QuotationTemplate

__all__ = ['JSONStorage', 'CSVStorage', 'GoogleSheetsStorage', 'QuotationTemplate']


storage/csv_storage.py"""CSV storage for catalog data."""

import csv
from typing import Dict, List


class CSVStorage:
    """Save catalog data as CSV files."""
    
    @staticmethod
    def catalog_to_rows(catalog: Dict) -> List[List[str]]:
        """
        Convert product catalog to CSV rows format:
        Categories | Component | References
        """
        rows = []
        
        for product_id, product_data in catalog.items():
            product_name = product_data['product_name']
            product_url = product_data['url']
            
            # Add product header row
            rows.append([f"Base Model({product_name})", product_name, product_url])
            
            # Add each customization category
            for category, options in product_data['customizations'].items():
                # First option in category includes category name
                if options:
                    first_option = options[0]
                    rows.append([
                        category,
                        first_option['label'],
                        first_option['image'] or ''
                    ])
                    
                    # Remaining options have empty category cell
                    for option in options[1:]:
                        rows.append([
                            '',
                            option['label'],
                            option['image'] or ''
                        ])
        
        return rows
    
    @staticmethod
    def save_simple(catalog: Dict, filepath: str):
        """Save catalog as simple CSV."""
        rows = CSVStorage.catalog_to_rows(catalog)
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Categories', 'Component', 'References'])
            writer.writerows(rows)
        
        print(f"âœ“ Saved {len(rows)} rows to {filepath}")
    
    @staticmethod
    def save_with_prices(catalog: Dict, filepath: str):
        """
        Save in a format with prices and additional metadata.
        """
        rows = []
        
        for product_id, product_data in catalog.items():
            product_name = product_data['product_name']
            base_price = product_data['base_price'] or 'N/A'
            product_url = product_data['url']
            
            # Product header
            rows.append([
                f"Base Model({product_name})",
                product_name,
                base_price,
                product_url,
                ''  # Notes column
            ])
            
            # Customization categories
            for category, options in product_data['customizations'].items():
                for i, option in enumerate(options):
                    rows.append([
                        category if i == 0 else '',
                        option['label'],
                        option['price'] or '',
                        option['image'] or '',
                        f"Category: {category}"
                    ])
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Categories', 'Component', 'Price', 'References', 'Notes'])
            writer.writerows(rows)
        
        print(f"âœ“ Saved {len(rows)} rows to {filepath} (with prices)")


storage/google_sheets.py"""Google Sheets integration for catalog data."""

import os
from typing import Dict, List, Optional
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError


class GoogleSheetsStorage:
    """Upload catalog data to Google Sheets."""
    
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
    
    def __init__(self, credentials_file: str = 'credentials.json'):
        """
        Initialize Google Sheets client.
        
        Args:
            credentials_file: Path to service account credentials JSON
        """
        self.credentials_file = credentials_file
        self.service = None
        self._authenticate()
    
    def _authenticate(self):
        """Authenticate with Google Sheets API."""
        try:
            if not os.path.exists(self.credentials_file):
                print(f"âš  Credentials file not found: {self.credentials_file}")
                print("  Please download service account credentials from Google Cloud Console")
                return
            
            creds = Credentials.from_service_account_file(
                self.credentials_file,
                scopes=self.SCOPES
            )
            self.service = build('sheets', 'v4', credentials=creds)
            print("âœ“ Google Sheets authenticated successfully")
        except Exception as e:
            print(f"âš  Failed to authenticate with Google Sheets: {e}")
            self.service = None
    
    def create_spreadsheet(self, title: str) -> Optional[str]:
        """
        Create a new Google Spreadsheet.
        
        Args:
            title: Title for the spreadsheet
            
        Returns:
            Spreadsheet ID or None if failed
        """
        if not self.service:
            print("âš  Google Sheets service not available")
            return None
        
        try:
            spreadsheet = {
                'properties': {
                    'title': title
                }
            }
            
            spreadsheet = self.service.spreadsheets().create(
                body=spreadsheet,
                fields='spreadsheetId'
            ).execute()
            
            spreadsheet_id = spreadsheet.get('spreadsheetId')
            print(f"âœ“ Created spreadsheet: {spreadsheet_id}")
            print(f"  URL: https://docs.google.com/spreadsheets/d/{spreadsheet_id}")
            
            return spreadsheet_id
        except HttpError as e:
            print(f"âœ— Failed to create spreadsheet: {e}")
            return None
    
    def upload_data(
        self,
        spreadsheet_id: str,
        data: List[List],
        sheet_name: str = 'Product Catalog',
        header_row: Optional[List[str]] = None
    ) -> bool:
        """
        Upload data to a Google Sheet.
        
        Args:
            spreadsheet_id: The spreadsheet ID
            data: List of rows to upload
            sheet_name: Name of the sheet tab
            header_row: Optional header row
            
        Returns:
            True if successful, False otherwise
        """
        if not self.service:
            print("âš  Google Sheets service not available")
            return False
        
        try:
            # Prepare data with header
            all_data = []
            if header_row:
                all_data.append(header_row)
            all_data.extend(data)
            
            # Clear existing data
            self.service.spreadsheets().values().clear(
                spreadsheetId=spreadsheet_id,
                range=f'{sheet_name}!A:Z'
            ).execute()
            
            # Upload new data
            body = {
                'values': all_data
            }
            
            result = self.service.spreadsheets().values().update(
                spreadsheetId=spreadsheet_id,
                range=f'{sheet_name}!A1',
                valueInputOption='RAW',
                body=body
            ).execute()
            
            print(f"âœ“ Uploaded {result.get('updatedCells')} cells to Google Sheets")
            return True
            
        except HttpError as e:
            print(f"âœ— Failed to upload data: {e}")
            return False
    
    def format_sheet(self, spreadsheet_id: str, sheet_name: str = 'Product Catalog'):
        """
        Apply formatting to the sheet (bold headers, freeze rows, etc.).
        
        Args:
            spreadsheet_id: The spreadsheet ID
            sheet_name: Name of the sheet tab
        """
        if not self.service:
            return
        
        try:
            # Get sheet ID
            spreadsheet = self.service.spreadsheets().get(
                spreadsheetId=spreadsheet_id
            ).execute()
            
            sheet_id = None
            for sheet in spreadsheet['sheets']:
                if sheet['properties']['title'] == sheet_name:
                    sheet_id = sheet['properties']['sheetId']
                    break
            
            if sheet_id is None:
                return
            
            # Format requests
            requests = [
                # Freeze header row
                {
                    'updateSheetProperties': {
                        'properties': {
                            'sheetId': sheet_id,
                            'gridProperties': {
                                'frozenRowCount': 1
                            }
                        },
                        'fields': 'gridProperties.frozenRowCount'
                    }
                },
                # Bold header row
                {
                    'repeatCell': {
                        'range': {
                            'sheetId': sheet_id,
                            'startRowIndex': 0,
                            'endRowIndex': 1
                        },
                        'cell': {
                            'userEnteredFormat': {
                                'textFormat': {
                                    'bold': True
                                }
                            }
                        },
                        'fields': 'userEnteredFormat.textFormat.bold'
                    }
                },
                # Auto-resize columns
                {
                    'autoResizeDimensions': {
                        'dimensions': {
                            'sheetId': sheet_id,
                            'dimension': 'COLUMNS',
                            'startIndex': 0,
                            'endIndex': 5
                        }
                    }
                }
            ]
            
            body = {
                'requests': requests
            }
            
            self.service.spreadsheets().batchUpdate(
                spreadsheetId=spreadsheet_id,
                body=body
            ).execute()
            
            print("âœ“ Applied formatting to sheet")
            
        except HttpError as e:
            print(f"âš  Failed to format sheet: {e}")
    
    def save_catalog(
        self,
        catalog: Dict,
        spreadsheet_id: Optional[str] = None,
        title: str = "Theraluxe Product Catalog",
        include_prices: bool = True
    ) -> Optional[str]:
        """
        Save catalog to Google Sheets.
        
        Args:
            catalog: The product catalog
            spreadsheet_id: Existing spreadsheet ID (creates new if None)
            title: Title for new spreadsheet
            include_prices: Whether to include prices column
            
        Returns:
            Spreadsheet ID or None if failed
        """
        if not self.service:
            print("âš  Cannot save to Google Sheets - service not available")
            return None
        
        # Create spreadsheet if needed
        if not spreadsheet_id:
            spreadsheet_id = self.create_spreadsheet(title)
            if not spreadsheet_id:
                return None
        
        # Prepare data
        rows = []
        
        for product_id, product_data in catalog.items():
            product_name = product_data['product_name']
            base_price = product_data['base_price'] or 'N/A'
            product_url = product_data['url']
            
            # Product header
            if include_prices:
                rows.append([
                    f"Base Model({product_name})",
                    product_name,
                    base_price,
                    product_url,
                    ''
                ])
            else:
                rows.append([
                    f"Base Model({product_name})",
                    product_name,
                    product_url
                ])
            
            # Customization categories
            for category, options in product_data['customizations'].items():
                for i, option in enumerate(options):
                    if include_prices:
                        rows.append([
                            category if i == 0 else '',
                            option['label'],
                            option['price'] or '',
                            option['image'] or '',
                            f"Category: {category}"
                        ])
                    else:
                        rows.append([
                            category if i == 0 else '',
                            option['label'],
                            option['image'] or ''
                        ])
        
        # Header row
        header = ['Categories', 'Component', 'Price', 'References', 'Notes'] if include_prices else ['Categories', 'Component', 'References']
        
        # Upload data
        success = self.upload_data(spreadsheet_id, rows, header_row=header)
        
        if success:
            # Apply formatting
            self.format_sheet(spreadsheet_id)
            print(f"\nâœ“ Catalog uploaded to Google Sheets")
            print(f"  URL: https://docs.google.com/spreadsheets/d/{spreadsheet_id}\n")
            return spreadsheet_id
        
        return None


storage/json_storage.py

"""JSON storage for catalog data."""

import json
from typing import Dict


class JSONStorage:
    """Save and load catalog data as JSON."""
    
    @staticmethod
    def save(catalog: Dict, filepath: str):
        """
        Save catalog to JSON file.
        
        Args:
            catalog: The catalog data
            filepath: Path to save the file
        """
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(catalog, f, indent=2, ensure_ascii=False)
        
        print(f"\n{'='*80}")
        print(f"âœ“ Saved catalog to {filepath}")
        print(f"{'='*80}\n")
    
    @staticmethod
    def load(filepath: str) -> Dict:
        """
        Load catalog from JSON file.
        
        Args:
            filepath: Path to the JSON file
            
        Returns:
            Catalog data
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)


storage/quotation_template.py"""Quotation template generator."""

import json
from typing import Dict


class QuotationTemplate:
    """Generate quotation templates for sales team."""
    
    @staticmethod
    def create(catalog: Dict, filepath: str):
        """
        Create a template that sales team can use for quotations.
        
        Args:
            catalog: The product catalog
            filepath: Where to save the template
        """
        template = {
            "products": {},
            "instructions": "Use this template to generate quotations. Select options for each category."
        }
        
        for product_id, product_data in catalog.items():
            template["products"][product_id] = {
                "product_name": product_data['product_name'],
                "base_price": product_data['base_price'],
                "url": product_data['url'],
                "customizations": {}
            }
            
            for category, options in product_data['customizations'].items():
                template["products"][product_id]["customizations"][category] = {
                    "options": [
                        {
                            "label": opt['label'],
                            "price": opt['price'],
                            "image": opt['image'],
                            "selected": False  # Sales team can toggle this
                        }
                        for opt in options
                    ]
                }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(template, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ Saved quotation template to {filepath}")


main.py
"""Main entry point for the scraper."""

import argparse
from .core.config import ScraperConfig
from .core.scraper import TheraluxeScraper


def main():
    """Main function to run the scraper."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Scrape Theraluxe product catalog',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--ai',
        action='store_true',
        help='Use AI (Gemini) for page classification'
    )
    
    parser.add_argument(
        '--max-pages',
        type=int,
        default=50,
        help='Maximum pages to crawl (default: 50)'
    )
    
    parser.add_argument(
        '--max-depth',
        type=int,
        default=3,
        help='Maximum crawl depth (default: 3)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='product_catalog.json',
        help='Output filename (default: product_catalog.json)'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=0.5,
        help='Delay between requests in seconds (default: 0.5)'
    )
    
    args = parser.parse_args()
    
    # Create configuration
    config = ScraperConfig(
        use_ai_classification=args.ai,
        max_pages=args.max_pages,
        max_depth=args.max_depth,
        output_filename=args.output,
        crawl_delay=args.delay
    )
    
    # Validate configuration
    if not config.validate():
        print("âš  Warning: Configuration validation failed, continuing anyway...")
    
    # Initialize scraper
    scraper = TheraluxeScraper(config)
    
    # Scrape all products
    catalog = scraper.scrape_all_products()
    
    # Save to file
    scraper.save_catalog(catalog)
    
    # Print summary
    scraper.print_summary(catalog)
    
    print(f"âœ“ Done! Check {config.full_output_path} for the complete catalog.\n")


if __name__ == "__main__":
    main()

run.py"""Alternative entry point - run this directly with: python run.py"""

import sys
import os

# Add the project root to the Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.main import main

if __name__ == "__main__":
    main()

setup.py"""Setup file for the package."""

from setuptools import setup, find_packages

setup(
    name="theraluxe-scraper",
    version="1.0.0",
    packages=find_packages(),
    install_requires=[
        "requests>=2.31.0",
        "python-dotenv>=1.0.0",
        "google-generativeai>=0.3.0",
    ],
    entry_points={
        'console_scripts': [
            'theraluxe-scraper=src.main:main',
        ],
    },
    author="Your Name",
    description="A modular web scraper for Theraluxe product catalog",
    python_requires='>=3.7',
)



theraluxe_scraper/
â”‚
â”œâ”€â”€ ğŸ“„ run.py                           # Main entry point (run this!)
â”œâ”€â”€ ğŸ“„ setup.py                         # Package installation
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Dependencies
â”œâ”€â”€ ğŸ“„ README.md                        # Main documentation
â”œâ”€â”€ ğŸ“„ GOOGLE_SHEETS_SETUP.md          # Google Sheets setup guide
â”œâ”€â”€ ğŸ“„ USAGE_EXAMPLES.md               # Usage examples
â”œâ”€â”€ ğŸ“„ .env                            # Environment variables (create this)
â”œâ”€â”€ ğŸ“„ .gitignore                      # Git ignore file
â”œâ”€â”€ ğŸ“„ credentials.json                # Google service account (download)
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ main.py                     # CLI entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ core/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ config.py               # Configuration management
â”‚   â”‚   â””â”€â”€ ğŸ“„ scraper.py              # Main scraper orchestrator
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ crawlers/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ web_crawler.py          # Website crawling logic
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ classifiers/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ base_classifier.py      # Base classifier interface
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rule_based.py           # Rule-based classification
â”‚   â”‚   â””â”€â”€ ğŸ“„ ai_classifier.py        # AI-based classification (Gemini)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ extractors/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ link_extractor.py       # URL extraction
â”‚   â”‚   â””â”€â”€ ğŸ“„ product_extractor.py    # Product data extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ storage/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ json_storage.py         # JSON export
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ csv_storage.py          # CSV export
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ google_sheets.py        # Google Sheets integration
â”‚   â”‚   â””â”€â”€ ğŸ“„ quotation_template.py   # Quotation template generator
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ utils/
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ http_client.py          # HTTP requests wrapper
â”‚       â””â”€â”€ ğŸ“„ url_utils.py            # URL manipulation
â”‚
â”œâ”€â”€ ğŸ“ data/                            # Output directory (auto-created)
â”‚   â””â”€â”€ ğŸ“ catalogs/
â”‚       â”œâ”€â”€ ğŸ“„ product_catalog.json
â”‚       â”œâ”€â”€ ğŸ“„ product_catalog.csv
â”‚       â”œâ”€â”€ ğŸ“„ product_catalog_with_prices.csv
â”‚       â””â”€â”€ ğŸ“„ product_catalog_quotation_template.json
â”‚
â””â”€â”€ ğŸ“ tests/                           # Unit tests (future)
    â”œâ”€â”€ ğŸ“„ __init__.py
    â”œâ”€â”€ ğŸ“ test_crawlers/
    â”œâ”€â”€ ğŸ“ test_classifiers/
    â”œâ”€â”€ ğŸ“ test_extractors/
    â””â”€â”€ ğŸ“ test_storage/


=============================================================================
KEY FILES TO CREATE MANUALLY:
=============================================================================

1. .env (Environment variables)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   GEMINI_API_KEY=your_api_key_here
   GOOGLE_CREDENTIALS_FILE=credentials.json
   GOOGLE_SPREADSHEET_ID=optional_spreadsheet_id


2. credentials.json (Google service account - download from Google Cloud)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   See GOOGLE_SHEETS_SETUP.md for instructions


3. .gitignore
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   # Environment
   .env
   credentials.json
   
   # Python
   __pycache__/
   *.py[cod]
   *$py.class
   *.so
   .Python
   
   # Virtual Environment
   venv/
   env/
   ENV/
   
   # IDE
   .vscode/
   .idea/
   *.swp
   *.swo
   
   # Data
   data/
   *.csv
   *.json
   !requirements.txt
   !package.json
   
   # OS
   .DS_Store
   Thumbs.db


=============================================================================
QUICK START:
=============================================================================

1. Install dependencies:
   pip install -r requirements.txt

2. Run basic scrape:
   python run.py

3. Export to CSV:
   python run.py --export csv

4. Export everything:
   python run.py --export all

5. With Google Sheets (after setup):
   python run.py --export google_sheets


=============================================================================
FEATURES BY MODULE:
=============================================================================

Storage Backends:
  âœ… JSON - Complete product catalog
  âœ… CSV - Simple format (Categories, Component, References)
  âœ… CSV with Prices - Includes pricing column
  âœ… Google Sheets - Direct upload to Google Sheets
  âœ… Quotation Template - JSON template for quotation system

Classifiers:
  âœ… Rule-based - Fast, no API needed
  âœ… AI-based (Gemini) - More accurate, requires API key

Extractors:
  âœ… Link Extraction - Find all internal links
  âœ… Product Extraction - Names, prices, customizations
  âœ… Price Extraction - Base prices and option prices
  âœ… Customization Extraction - All options with images

Utilities:
  âœ… HTTP Client - Jina AI integration
  âœ… URL Utils - URL manipulation and validation






utils/http_client.py
"""HTTP client for making requests."""

import requests
from typing import Optional, Dict


class HTTPClient:
    """Wrapper for HTTP requests with Jina AI integration."""
    
    def __init__(self, timeout: int = 15):
        self.timeout = timeout
        self.jina_base_url = "https://r.jina.ai/"
    
    def scrape_with_jina(self, url: str) -> Optional[str]:
        """
        Scrape a URL using Jina AI Reader.
        
        Args:
            url: The URL to scrape
            
        Returns:
            Markdown content or None if failed
        """
        try:
            headers = {
                "X-Return-Format": "markdown",
                "X-Remove-Selector": "nav, footer, header, .navigation, .menu, script, style, .cookie"
            }
            
            response = requests.get(
                f"{self.jina_base_url}{url}",
                headers=headers,
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.text
            
        except requests.exceptions.Timeout:
            print(f"  âœ— Timeout scraping {url}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"  âœ— Failed to scrape {url}: {e}")
            return None
    
    def get(self, url: str, headers: Optional[Dict] = None) -> Optional[requests.Response]:
        """
        Make a GET request.
        
        Args:
            url: The URL to request
            headers: Optional headers
            
        Returns:
            Response object or None if failed
        """
        try:
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"  âœ— Request failed for {url}: {e}")
            return None

utils/__init__
"""Utility modules."""

from .http_client import HTTPClient
from .url_utils import URLUtils

__all__ = ['HTTPClient', 'URLUtils']


url_utils.py
"""URL manipulation utilities."""

from urllib.parse import urljoin, urlparse
from typing import Set


class URLUtils:
    """Utilities for URL manipulation and validation."""
    
    @staticmethod
    def is_same_domain(url1: str, url2: str) -> bool:
        """Check if two URLs are from the same domain."""
        return urlparse(url1).netloc == urlparse(url2).netloc
    
    @staticmethod
    def clean_url(url: str) -> str:
        """
        Clean URL by removing fragments and query parameters.
        
        Args:
            url: The URL to clean
            
        Returns:
            Cleaned URL
        """
        return url.split('#')[0].split('?')[0]
    
    @staticmethod
    def make_absolute(url: str, base_url: str) -> str:
        """
        Convert relative URL to absolute URL.
        
        Args:
            url: The URL (relative or absolute)
            base_url: The base URL
            
        Returns:
            Absolute URL
        """
        if not url.startswith('http'):
            return urljoin(base_url, url)
        return url
    
    @staticmethod
    def is_valid_url(url: str) -> bool:
        """
        Check if URL is valid and not an anchor or mailto link.
        
        Args:
            url: The URL to validate
            
        Returns:
            True if valid, False otherwise
        """
        if url.startswith('#') or url.startswith('mailto:'):
            return False
        return True
    
    @staticmethod
    def get_path_segment(url: str, segment: int = -1) -> str:
        """
        Get a specific path segment from URL.
        
        Args:
            url: The URL
            segment: Which segment to get (default: last)
            
        Returns:
            Path segment
        """
        path = urlparse(url).path
        segments = [s for s in path.split('/') if s]
        if segments:
            return segments[segment]
        return ""

