---
title: Product Catalogue AI
emoji: ğŸ“ˆ
colorFrom: red
colorTo: green
sdk: docker
pinned: false
---

# ğŸ›ï¸ Product Catalogue AI

An intelligent web scraper for extracting product catalogs with customization options from e-commerce websites. Features hybrid static/dynamic extraction with automatic model selection.

## ğŸŒŸ Features

- **ğŸ¯ Dual Extraction Models**
  - **Model-S**: Fast static extraction using Jina AI markdown conversion
  - **Model-D**: Browser-based dynamic extraction for JavaScript configurators
  - **Hybrid Mode**: Automatic intelligent routing between S and D per page

- **ğŸ§  Smart Classification**
  - Balanced classification with configurable strictness (lenient/balanced/strict)
  - Automatic detection of product pages, categories, and blog posts
  - Dynamic configurator detection for complex JavaScript-driven products

- **ğŸ¨ Advanced Customization Extraction**
  - Color extraction with HEX values from swatches and images
  - External configurator support (Shopify, WooCommerce, etc.)
  - Embedded configurator detection and navigation
  - Price variants and option discovery

- **ğŸ“Š Multiple Export Formats**
  - JSON (structured catalog)
  - CSV (product list and pricing matrix)
  - Quotation templates
  - Google Sheets integration

## ğŸ—ï¸ Architecture

### Core Components

```
src/
â”œâ”€â”€ classifiers/          # Page classification and routing
â”‚   â”œâ”€â”€ balanced_classifier.py    # Static page classification (Model-S)
â”‚   â”œâ”€â”€ dynamic_classifier.py     # Hybrid routing (S/D selection)
â”‚   â””â”€â”€ base_classifier.py        # Base classification interface
â”‚
â”œâ”€â”€ core/                 # Main scraper engines
â”‚   â”œâ”€â”€ balanced_scraper.py       # Model-S: Static extraction
â”‚   â”œâ”€â”€ dynamic_scraper.py        # Model-D: Hybrid extraction
â”‚   â””â”€â”€ config.py                 # Configuration management
â”‚
â”œâ”€â”€ crawlers/            # Web crawling
â”‚   â””â”€â”€ web_crawler.py            # Page discovery with classification
â”‚
â”œâ”€â”€ dynamic/             # Browser automation (Model-D)
â”‚   â”œâ”€â”€ browser_engine.py         # Playwright browser control
â”‚   â”œâ”€â”€ dynamic_detector.py       # JS configurator detection
â”‚   â”œâ”€â”€ option_discovery.py       # Interactive control detection
â”‚   â”œâ”€â”€ network_capture.py        # API request monitoring
â”‚   â””â”€â”€ price_learner.py          # Dynamic pricing analysis
â”‚
â”œâ”€â”€ extractors/          # Data extraction
â”‚   â”œâ”€â”€ product_extractor.py      # Product info extraction
â”‚   â”œâ”€â”€ configurator_detector.py  # Configurator detection logic
â”‚   â”œâ”€â”€ external_configurator_scraper.py  # External platforms
â”‚   â”œâ”€â”€ color_extractor.py        # Color analysis
â”‚   â”œâ”€â”€ color_normalizer.py       # Color name standardization
â”‚   â”œâ”€â”€ color_sampler.py          # Image color sampling
â”‚   â”œâ”€â”€ swatch_detector.py        # Color swatch detection
â”‚   â””â”€â”€ link_extractor.py         # URL discovery
â”‚
â”œâ”€â”€ storage/             # Output handlers
â”‚   â”œâ”€â”€ json_storage.py           # JSON export
â”‚   â”œâ”€â”€ csv_storage.py            # CSV export
â”‚   â”œâ”€â”€ google_sheets.py          # Google Sheets integration
â”‚   â””â”€â”€ quotation_template.py    # Quote generation
â”‚
â””â”€â”€ utils/               # Utilities
    â”œâ”€â”€ http_client.py            # HTTP requests + Jina AI
    â””â”€â”€ url_utils.py              # URL manipulation
```

### Data Flow

```
1. Entry Point (main.py)
   â†“
2. Configuration (ScraperConfig)
   â†“
3. Scraper Selection
   â”œâ”€â†’ Model-S (BalancedScraper)
   â”‚   â””â”€â†’ BalancedClassifier
   â”‚
   â””â”€â†’ Model-D (DynamicScraper)
       â””â”€â†’ DynamicClassifier (routes to S or D per page)
   â†“
4. Web Crawler (WebCrawler)
   - Discovers pages via link extraction
   - Classifies each page (product/category/blog/other)
   - Routes to appropriate model
   â†“
5. Extraction
   â”œâ”€â†’ Static (ProductExtractor + ConfiguratorDetector)
   â”‚   - Jina AI markdown conversion
   â”‚   - Pattern-based extraction
   â”‚   - External configurator following
   â”‚
   â””â”€â†’ Dynamic (BrowserRunner + OptionDiscovery)
       - Playwright browser automation
       - Interactive control detection
       - Network activity monitoring
       - Price learning
   â†“
6. Storage (JSONStorage, CSVStorage, etc.)
   - Multiple format export
   - Google Sheets sync
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone <repo-url>
cd product-catalogue-ai

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers (for Model-D)
playwright install chromium
```

### Basic Usage

```bash
# Hybrid mode (recommended) - auto-selects Model-S or Model-D
python main.py --url https://example.com/products

# Static extraction only (Model-S) - fast
python main.py --url https://example.com/products --model S

# Hybrid mode (Model-D) - handles JS configurators
python main.py --url https://example.com/products --model D

# With custom settings
python main.py --url https://example.com/products \
  --strictness balanced \
  --max-pages 100 \
  --max-depth 4 \
  --export all
```

## ğŸ›ï¸ Configuration

### Model Selection

| Model | Description | Use Case | Speed |
|-------|-------------|----------|-------|
| **S** | Static extraction only | Standard product pages, no JS | âš¡ Fast |
| **D** | Hybrid auto-selection | Mixed sites with JS configurators | ğŸ¢ Slow |
| **auto** | Same as D (default) | Recommended for all sites | ğŸ¢ Slow |

### Strictness Levels

| Level | Precision | Recall | Description |
|-------|-----------|--------|-------------|
| **lenient** | Low | High | Catches everything, some false positives |
| **balanced** | Medium | Medium | **Recommended** - good balance |
| **strict** | High | Low | Very clean results, may miss products |

### Classification Logic

**Model-S (Static Classification)**:
- URL patterns (product keywords, SKU patterns)
- Content signals (price elements, customization keywords)
- Structure analysis (option categories, checkboxes)
- CTA detection (Add to Cart, Buy Now)
- Negative signals (blog indicators, article structure)

**Model-D (Dynamic Detection)**:
- JavaScript framework detection (React, Vue, Angular)
- SPA indicators (single-page app patterns)
- Dynamic pricing signals (calculator, price-update)
- **Critical Signal**: Price present + NO static options â†’ likely JS configurator
- Known platforms (Shopify apps, WooCommerce plugins)

**Routing Decision**:
```python
if price_found and not static_options_found:
    confidence += 0.50  # Strong signal for Model-D

if confidence >= 0.50:
    use Model-D (browser automation)
else:
    use Model-S (static extraction)
```

## ğŸ“‹ Export Formats

```bash
# Single format
python main.py --url <url> --export json

# Multiple formats
python main.py --url <url> --export json,csv,quotation

# All formats
python main.py --url <url> --export all
```

**Available formats**:
- `json` - Structured catalog with full metadata
- `csv` - Product list with basic info
- `csv_prices` - Pricing matrix with variants
- `quotation` - Quote template with categories
- `google_sheets` - Direct Google Sheets sync

## ğŸ”§ Advanced Configuration

### Environment Variables

Create a `.env` file:

```env
# Optional: Default URL
BASE_URL=https://example.com/products

# Optional: AI features (future)
GEMINI_API_KEY=your_gemini_api_key

# Optional: Google Sheets export
GOOGLE_CREDENTIALS_FILE=credentials.json
GOOGLE_SPREADSHEET_ID=your_sheet_id
```

### Browser Settings (Model-D)

```bash
# Visible browser for debugging
python main.py --url <url> --model D --no-headless

# Adjust timeouts
# Edit src/core/dynamic_scraper.py:
self.browser_config = BrowserConfig(
    headless=True,
    timeout=60000,  # 60 seconds
    wait_after_action=1000
)
```

## ğŸ” Troubleshooting

### Issue: Model-D timing out

**Solution**: Page may be too slow. Check browser settings:
```python
# In browser_engine.py, already set to "domcontentloaded"
await self.page.goto(url, wait_until="domcontentloaded")
```

### Issue: No products found

**Solutions**:
1. Try lenient mode: `--strictness lenient`
2. Increase crawl limits: `--max-pages 100 --max-depth 5`
3. Check if site blocks scrapers (add delays: `--delay 2.0`)

### Issue: Missing customization options

**Static pages (Model-S)**: Options must match patterns:
- Images with prices: `![Option (+$50)](url)`
- Checkboxes: `- [x] Option (+$50)`
- Categories: `Size:` followed by options

**Dynamic pages (Model-D)**: Browser automation finds interactive controls automatically

### Issue: Unicode errors on Windows

**Solution**: Already handled in main.py:
```python
sys.stdout.reconfigure(encoding='utf-8')
```

## ğŸ“Š Output Structure

### JSON Format

```json
{
  "product_1": {
    "product_name": "Custom Sauna",
    "url": "https://example.com/product",
    "base_price": "$1000",
    "model": "D",
    "extraction_method": "dynamic_browser",
    "has_configurator": true,
    "configurator_type": "dynamic",
    "customization_categories": ["Size", "Wood Type", "Heater"],
    "customizations": {
      "Size": [
        {"label": "6x8", "price": "+$500", "image": null}
      ]
    },
    "total_customization_options": 15
  }
}
```

## ğŸ› ï¸ Development

### Adding New Extractors

1. Create extractor in `src/extractors/`
2. Import in `ProductExtractor` or `DynamicScraper`
3. Add logic to `extract_customizations()` or `_extract_dynamic()`

### Adding Export Formats

1. Create storage handler in `src/storage/`
2. Add to `save_catalog()` in scraper
3. Update CLI arguments in `main.py`

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ğŸ“ Support

For issues and questions, please open a GitHub issue.

---

**Built with â¤ï¸ using Python, Playwright, and Jina AI**
